c('This view is horrible', 'negative'),
c('I feel tired this morning', 'negative'),
c('I am not looking forward to the concert', 'negative'),
c('He is my enemy', 'negative')
)
test_tweets = rbind(
c('feel happy this morning', 'positive'),
c('larry friend', 'positive'),
c('not like that man', 'negative'),
c('house not great', 'negative'),
c('your song annoying', 'negative')
)
tweets = rbind(pos_tweets, neg_tweets, test_tweets)
# build dtm
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:10,], as.factor(tweets[1:10,2]) )
# test the validity
predicted = predict(classifier, mat[11:15,]); predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
install.packages("RTextTools")
library(RTextTools)
library(e1071)
pos_tweets =  rbind(
c('I love this car', 'positive'),
c('This view is amazing', 'positive'),
c('I feel great this morning', 'positive'),
c('I am so excited about the concert', 'positive'),
c('He is my best friend', 'positive')
)
neg_tweets = rbind(
c('I do not like this car', 'negative'),
c('This view is horrible', 'negative'),
c('I feel tired this morning', 'negative'),
c('I am not looking forward to the concert', 'negative'),
c('He is my enemy', 'negative')
)
test_tweets = rbind(
c('feel happy this morning', 'positive'),
c('larry friend', 'positive'),
c('not like that man', 'negative'),
c('house not great', 'negative'),
c('your song annoying', 'negative')
)
tweets = rbind(pos_tweets, neg_tweets, test_tweets)
# build dtm
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:10,], as.factor(tweets[1:10,2]) )
# test the validity
predicted = predict(classifier, mat[11:15,]); predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
pos_tweets <- train["sentiment" = 1,]
pos_tweets <- train["sentiment" == 1,]
train["sentiment" == 1,]
pos_tweets <- train["sentiment" == TRUE,]
pos_tweets <- train[which(train$sentiment == TRUE),]
neg_tweets <- train[which(train$sentiment == FALSE),]
pos_tweets <- train[which(train$sentiment == TRUE), 3]
neg_tweets <- train[which(train$sentiment == FALSE), 3]
pos_tweets <- train[which(train$sentiment == TRUE), c(3, 2)]
neg_tweets <- train[which(train$sentiment == FALSE), c(3, 2)]
tweets = rbind(pos_tweets, neg_tweets, test_tweets)
# build dtm
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:10,], as.factor(tweets[1:10,2]) )
# test the validity
predicted = predict(classifier, mat[11:15,]); predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
tweets = rbind(pos_tweets, neg_tweets, test_tweets)
# build dtm
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[,], as.factor(tweets[,2]) )
# test the validity
predicted = predict(classifier, mat[11:15,]); predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
names(test_tweets) = names(pos_tweets)
tweets = rbind(pos_tweets, neg_tweets, test_tweets)
# build dtm
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[,], as.factor(tweets[,2]) )
# test the validity
predicted = predict(classifier, mat[11:15,]); predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
test_tweets = rbind(
c('feel happy this morning', 1),
c('larry friend', 1),
c('not like that man', 0),
c('house not great', 0),
c('your song annoying', 0)
)
names(test_tweets) = names(pos_tweets)
tweets = rbind(pos_tweets, neg_tweets, test_tweets)
# build dtm
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[,], as.factor(tweets[,2]) )
# test the validity
predicted = predict(classifier, mat[11:15,]); predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
matrix= create_matrix(test[,3], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[,], as.factor(tweets[,2]) )
# test the validity
predicted = predict(classifier, mat[11:15,]); predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
matrix= create_matrix(test[,3], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[,], as.factor(tweets[,2]) )
# test the validity
predicted = predict(classifier, mat[1:100,]); predicted
table(test[1:100, 2], predicted)
recall_accuracy(test[1:100, 2], predicted)
tweets = rbind(pos_tweets, neg_tweets, test_tweets)
# build dtm
matrix= create_matrix(test[1:100,3], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[,], as.factor(tweets[,2]) )
# test the validity
predicted = predict(classifier, mat[1:100,]); predicted
table(test[1:100, 2], predicted)
recall_accuracy(test[1:100, 2], predicted)
tweets = rbind(pos_tweets, neg_tweets, test[, c(3,2)])
tweets = rbind(pos_tweets, neg_tweets, test[, c(3,2)])
# build dtm
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[,], as.factor(tweets[,2]) )
tweets = rbind(pos_tweets, neg_tweets, test[, c(3,2)])
# build dtm
matrix= create_matrix(tweets[1:100,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[,], as.factor(tweets[,2]) )
classifier = naiveBayes(mat[,], as.factor(tweets[1:100,2]) )
predicted = predict(classifier, mat[1:100,]); predicted
table(tweets[1:100, 2], predicted)
recall_accuracy(tweets[1:100, 2], predicted)
predicted = predict(classifier, mat[,]); predicted
table(tweets[14000:14100, 2], predicted)
recall_accuracy(tweets[14000:14100, 2], predicted)
predicted = predict(classifier, mat[1:100,]); predicted
pos_tweets =  rbind(
c('I love this car', 'positive'),
c('This view is amazing', 'positive'),
c('I feel great this morning', 'positive'),
c('I am so excited about the concert', 'positive'),
c('He is my best friend', 'positive')
)
neg_tweets = rbind(
c('I do not like this car', 'negative'),
c('This view is horrible', 'negative'),
c('I feel tired this morning', 'negative'),
c('I am not looking forward to the concert', 'negative'),
c('He is my enemy', 'negative')
)
test_tweets = rbind(
c('feel happy this morning', 'positive'),
c('larry friend', 'positive'),
c('not like that man', 'negative'),
c('house not great', 'negative'),
c('your song annoying', 'negative')
)
tweets = rbind(pos_tweets, neg_tweets, test_tweets)
# build dtm
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:10,], as.factor(tweets[1:10,2]) )
# test the validity
predicted = predict(classifier, mat[11:15,]); predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
tweets = rbind(pos_tweets[1:100,], neg_tweets[1:100,], test[1:100, c(3,2)])
# build dtm
matrix= create_matrix(tweets[1:200,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:200,], as.factor(tweets[1:200,2]) )
tweets = rbind(pos_tweets[1:100,], neg_tweets[1:100,], test[1:100, c(3,2)])
pos_tweets <- train[which(train$sentiment == TRUE), c(3, 2)]
neg_tweets <- train[which(train$sentiment == FALSE), c(3, 2)]
tweets = rbind(pos_tweets[1:100,], neg_tweets[1:100,], test[1:100, c(3,2)])
matrix= create_matrix(tweets[1:200,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:200,], as.factor(tweets[1:200,2]) )
predicted = predict(classifier, mat[200:300,]); predicted
table(tweets[200:300, 2], predicted)
recall_accuracy(tweets[200:300, 2], predicted)
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:200,], as.factor(tweets[1:200,2]) )
predicted = predict(classifier, mat[200:300,]); predicted
table(tweets[200:300, 2], predicted)
recall_accuracy(tweets[200:300, 2], predicted)
tweets = rbind(pos_tweets[1:100,], neg_tweets[1:100,], train[100:200, c(3,2)])
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:200,], as.factor(tweets[1:200,2]) )
predicted = predict(classifier, mat[200:300,]); predicted
table(tweets[200:300, 2], predicted)
recall_accuracy(tweets[200:300, 2], predicted)
pos_tweets$sentiment <- 'positive'
neg_tweets$sentiment <- 'negative'
tweets = rbind(pos_tweets[1:100,], neg_tweets[1:100,], train[100:200, c(3,2)])
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:200,], as.factor(tweets[1:200,2]) )
predicted = predict(classifier, mat[200:300,]); predicted
recall_accuracy(tweets[200:300, 2], predicted)
tweets[200:202, 3]
tweets[200:202, 2]
pos_tweets <- train[which(train$sentiment == TRUE), c(3, 2)]
neg_tweets <- train[which(train$sentiment == FALSE), c(3, 2)]
tweets = rbind(pos_tweets[1:100,], neg_tweets[1:100,], train[100:200, c(3,2)])
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:200,], as.factor(tweets[1:200,2]) )
predicted = predict(classifier, mat[200:300,]); predicted
tweets[200:202, 2]
predicted = predict(classifier, mat[200:201,]); predicted
table(tweets[200:201, 2], predicted)
recall_accuracy(tweets[200:201, 2], predicted)
tweets[200:202, 2]
train_size <- 500
pos_tweets <- train[which(train$sentiment == TRUE), c(3, 2)]
pos_tweets <- pos_tweets[1:train_size,]
neg_tweets <- train[which(train$sentiment == FALSE), c(3, 2)]
neg_tweets <- neg_tweets[1:train_size,]
tweets = rbind(pos_tweets, neg_tweets,
train[14000:14100,], c(3,2)])
tweets = rbind(pos_tweets, neg_tweets,
train[14000:14100, c(3,2)])
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:train_size*2,], as.factor(tweets[1:train_size*2,2]) )
predicted = predict(classifier, mat[train_size*2:train_size*2+100,]); predicted
predicted = predict(classifier, mat[train_size*2:length(mat),]); predicted
predicted = predict(classifier, mat[1000:length(mat),]); predicted
length(mat)
predicted = predict(classifier, mat[1000:1100,]); predicted
mat[1001:1101,]
mat[11:15,]
pos_tweets =  rbind(
c('I love this car', 'positive'),
c('This view is amazing', 'positive'),
c('I feel great this morning', 'positive'),
c('I am so excited about the concert', 'positive'),
c('He is my best friend', 'positive')
)
neg_tweets = rbind(
c('I do not like this car', 'negative'),
c('This view is horrible', 'negative'),
c('I feel tired this morning', 'negative'),
c('I am not looking forward to the concert', 'negative'),
c('He is my enemy', 'negative')
)
test_tweets = rbind(
c('feel happy this morning', 'positive'),
c('larry friend', 'positive'),
c('not like that man', 'negative'),
c('house not great', 'negative'),
c('your song annoying', 'negative')
)
tweets = rbind(pos_tweets, neg_tweets, test_tweets)
# build dtm
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:10,], as.factor(tweets[1:10,2]) )
# test the validity
predicted = predict(classifier, mat[11:15,]); predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
mat[11:15,]
train_size <- 5
pos_tweets <- train[which(train$sentiment == TRUE), c(3, 2)]
pos_tweets <- pos_tweets[1:train_size,]
neg_tweets <- train[which(train$sentiment == FALSE), c(3, 2)]
neg_tweets <- neg_tweets[1:train_size,]
tweets = rbind(pos_tweets, neg_tweets,
train[14000:14100, c(3,2)])
# build dtm
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:train_size*2,], as.factor(tweets[1:train_size*2,2]) )
# test the validity
predicted = predict(classifier, mat[11:15,]); predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
tweets[11:15, 2]
View(tweets)
View(tweets)
pos_tweets <- train[which(train$sentiment == TRUE), c(3, 2)]
pos_tweets <- pos_tweets[1:train_size,]
neg_tweets <- train[which(train$sentiment == FALSE), c(3, 2)]
neg_tweets <- neg_tweets[1:train_size,]
tweets = rbind(pos_tweets, neg_tweets,
train[500:600, c(3,2)])
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:train_size*2,], as.factor(tweets[1:train_size*2,2]) )
# test the validity
predicted = predict(classifier, mat[11:15,]); predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
tweets[11:15, 2]
mat[11:15,]
mat[11,]
View(tweets)
train_size <- 3000
pos_tweets <- train[which(train$sentiment == TRUE), c(3, 2)]
pos_tweets <- pos_tweets[1:train_size,]
neg_tweets <- train[which(train$sentiment == FALSE), c(3, 2)]
neg_tweets <- neg_tweets[1:train_size,]
tweets = rbind(pos_tweets, neg_tweets,
train[500:600, c(3,2)])
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:train_size*2,], as.factor(tweets[1:train_size*2,2]) )
tweets[6001:6011, 2]
predicted = predict(classifier, mat[6001:6011,]); predicted
table(tweets[6001:6011, 2], predicted)
recall_accuracy(tweets[6001:6011, 2], predicted)
mat[6001,]
view(mat[6001,])
View(tweets)
View(tweets)
View(mat[6001,])
library(tm)
tokenize <- function(documents){
# Lowercase all words for convenience
doc <- tolower(documents$review)
# Remove all #hashtags and @mentions
doc <- gsub("(?:#|@)[a-zA-Z0-9_]+ ?", "", doc)
# Remove words with more than 3 numbers in them (they overwhelm the corpus, and are uninformative)
doc <- gsub("[a-zA-Z]*([0-9]{3,})[a-zA-Z0-9]* ?", "", doc)
# Remove all punctuation
doc <- gsub("[[:punct:]]", "", doc)
# Remove all newline characters
doc <- gsub("[\r\n]", "", doc)
# Regex pattern for removing stop words
stop_pattern <- paste0("\\b(", paste0(stopwords("en"), collapse="|"), ")\\b")
doc <- gsub(stop_pattern, "", doc)
# Replace whitespace longer than 1 space with a single space
doc <- gsub(" {2,}", " ", doc)
# # Split on spaces and return list of character vectors
# doc_words <- strsplit(doc, " ")
#
# doc_words <- data.frame(review = unlist(doc_words), stringsAsFactors = FALSE)
#
# doc_words <- doc_words[complete.cases(doc_words),]
#
# return(data.frame(review = unlist(doc_words), stringsAsFactors = FALSE))
return(doc)
}
tweets$review <- tokenize(tweets$review)
tweets[, 2] <- tokenize(tweets[,2])
tweets[, 2]
tweets[, 1] <- tokenize(tweets[,1])
tweets[,1]
tweets[, 1] <- tokenize(tweets[,1])
tweets$review <- tokenize(tweets$review)
tokenize <- function(documents){
# Lowercase all words for convenience
doc <- tolower(documents)
# Remove all #hashtags and @mentions
doc <- gsub("(?:#|@)[a-zA-Z0-9_]+ ?", "", doc)
# Remove words with more than 3 numbers in them (they overwhelm the corpus, and are uninformative)
doc <- gsub("[a-zA-Z]*([0-9]{3,})[a-zA-Z0-9]* ?", "", doc)
# Remove all punctuation
doc <- gsub("[[:punct:]]", "", doc)
# Remove all newline characters
doc <- gsub("[\r\n]", "", doc)
# Regex pattern for removing stop words
stop_pattern <- paste0("\\b(", paste0(stopwords("en"), collapse="|"), ")\\b")
doc <- gsub(stop_pattern, "", doc)
# Replace whitespace longer than 1 space with a single space
doc <- gsub(" {2,}", " ", doc)
# # Split on spaces and return list of character vectors
# doc_words <- strsplit(doc, " ")
#
# doc_words <- data.frame(review = unlist(doc_words), stringsAsFactors = FALSE)
#
# doc_words <- doc_words[complete.cases(doc_words),]
#
# return(data.frame(review = unlist(doc_words), stringsAsFactors = FALSE))
return(doc)
}
tweets$review <- tokenize(tweets$review)
View(tweets)
View(tweets)
if(is.null(corpus)){
doc_words <- strsplit(tweets$review, " ")
doc_words <- data.frame(review = unlist(doc_words), stringsAsFactors = FALSE)
doc_words <- doc_words[complete.cases(doc_words),]
corpus <- corpus_freq(data.frame(review = unlist(doc_words), stringsAsFactors = FALSE), corpus_size=corpus_size)
}
doc_words <- strsplit(tweets$review, " ")
doc_words <- data.frame(review = unlist(doc_words), stringsAsFactors = FALSE)
doc_words <- doc_words[complete.cases(doc_words),]
corpus <- corpus_freq(data.frame(review = unlist(doc_words), stringsAsFactors = FALSE), corpus_size=corpus_size)
corpus_freq <- function(tokens, corpus_size=NULL, word_list = NULL){
# Concatenate all tokenized words into a single character list
all_words <- do.call(c, tokens)
#If corpus size is not blank, and word list is, create a word frequency frame
#take the top occuring words up to the length of corpus_size
#and reorder alphabetically
#This gives us an data frame of the most frequent words in our corpus, ordered alphabetically
#sized by the corpus_size parameter
if(is.null(word_list) & !is.null(corpus_size)){
corpusfreq <- data.frame(table(all_words))
names(corpusfreq) <- c("Word", "Freq")
corpusfreq$Word <- as.character(corpusfreq$Word)
corpusfreq$Freq <- as.numeric(corpusfreq$Freq)
corpusfreq <- corpusfreq[order(-corpusfreq$Freq), ]
corpusfreq <- corpusfreq[1:corpus_size, ]
corpusfreq <- corpusfreq[order(corpusfreq$Word), ]
} else {
#Else it is assumed a pre-compiled word list has been passed into the function
corpusfreq <- data.frame(word_list)
names(corpusfreq) <- c("Word")
}
# N docs is where we will store the document frequency (I.E how many documents a word appears in)
# We'll need this to calculate TF-IDF
corpusfreq$n_docs <- 0
# For every vector of words in our tokenized list, count how many times each word in our corpus occurs
for(token_list in tokens){
t <- data.frame(table(token_list))
names(t) <- c("Word", "n_docs")
t$n_docs <- 1
t_freq <- merge(x=corpusfreq, y=t, by="Word", all.x=TRUE)
t_freq$n_docs.y[is.na(t_freq$n_docs.y)] <- 0
corpusfreq$n_docs <- corpusfreq$n_docs + t_freq$n_docs.y
}
return(corpusfreq)
}
corpus <- corpus_freq(data.frame(review = unlist(doc_words), stringsAsFactors = FALSE), corpus_size=corpus_size)
corpus <- corpus_freq(data.frame(review = unlist(doc_words), stringsAsFactors = FALSE), corpus_size=3000)
View(corpus)
View(corpus)
doc <- gsub(corpus, "", doc)
doc <- tweets$review
doc <- gsub(corpus, "", doc)
